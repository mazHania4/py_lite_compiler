
# PY_LITE_COMPILER 
A Python-Inspired Programming Language Analyzer

<!-- TOC -->
* [PY_LITE_COMPILER](#pylitecompiler-)
  * [Introduction](#introduction)
    * [General description of Features](#general-description-of-features)
  * [System Requirements](#system-requirements)
    * [Hardware Requirements](#hardware-requirements)
    * [Software Requirements](#software-requirements)
  * [Technologies Used](#technologies-used)
    * [Programming Language](#programming-language)
    * [User Interface Framework](#user-interface-framework)
    * [Dependency and Package Manager](#dependency-and-package-manager)
    * [Integrated Development Environment (IDE)](#integrated-development-environment-ide)
    * [External Libraries](#external-libraries)
* [Lexical Analysis](#lexical-analysis)
  * [Token Types](#token-types)
  * [Lexical Analysis Errors](#lexical-analysis-errors)
* [Syntax Analyzer](#syntax-analyzer)
  * [Supported Syntax Elements](#supported-syntax-elements)
  * [Syntax Analysis Process](#syntax-analysis-process)
  * [Syntax errors](#syntax-errors)
  * [User Interface Overview](#user-interface-overview)
    * [Code Editor](#code-editor)
      * [Top Toolbar](#top-toolbar)
    * [Reports](#reports)
      * [Token Table (row 1)](#token-table-row-1)
      * [Output Space (row 2)](#output-space-row-2)
      * [Functions and Blocks Tables (row 3)](#functions-and-blocks-tables-row-3)
<!-- TOC -->

## Introduction

Py_lite_compiler is a Python-Inspired Programming Language Analyzer,
is a tool designed to scan, parse and analyze code written in Python on a basic level. 
This tool consists of two primary components: a Lexical Analyzer and a Syntactic Analyzer. 
It is specifically designed to recognize and process basic code structures.

### General description of Features

- **Lexical Analysis:** 
The Lexical Analyzer is responsible for breaking down the input code into individual tokens, 
such as keywords, identifiers, literals, and other symbols that can be operators or delimiters. 
It ensures that the input code adheres to the lexical rules of the Python language.

- **Syntactic Analysis:** 
The Syntactic Analyzer takes the tokens generated by the Lexical Analyzer and checks 
whether they form valid syntactic structures. 
It enforces the grammar rules of the language, ensuring that the code is well-structured.

- **Basic Structure Recognition:** 
This analyzer is designed to identify and understand fundamental code constructs, 
including variable declarations, loops, conditionals, and function definitions. 
It provides insights into the structure of the code, and reports about certain found structures.

- **Code Validation:** 
It ensures that the Python code written is correctly structured and adheres to the language's syntax rules.

- **Error Detection:** 
The analyzer spots both lexical and syntax errors, like invalid characters, missing tokens, 
or other issues in the code, and also gives feedback to spot and fix the error.

- **Development in Python:** 
It can be used as a learning tool for developers interested in the Python 
language, as it assists in verifying the correctness of the language's grammar.

- **Learning Tool:** 
It can also mean a resource for students and beginners learning about parsing and language analysis.

To get started with the Python-Inspired Programming Language Analyzer, please refer to the 
usage instructions provided in this manual.

---

## System Requirements

Before using the Python-Inspired Programming Language Analyzer, it's essential 
to ensure that the computer meets the following hardware and software requirements:

### Hardware Requirements

1. **Computer:** A computer with standard hardware components its needed, 
including a CPU, RAM, storage, input devices, and a display.

2. **RAM Memory:** A minimum of 2 GB of RAM is required to run the application smoothly.

3. **Processor:** The computer should have a processor with a clock speed of 
at least 1.4 GHz to handle code analysis efficiently.

4. **Input Devices:** A keyboard and mouse are necessary for interacting with the application.

5. **Monitor:** A monitor or display unit is required to view the application's interface and analysis results.

### Software Requirements

1. **Operating System:** Py_lite_Compiler is compatible with various operating systems, including:
    - Windows 7 onwards
    - Commonly used Linux distributions
    - macOS

   Ensure that the chosen operating system is up-to-date with the latest updates and patches.

2. **Java 17.0:** The analyzer relies on Java 17.0 or a later compatible version.
Worth mentioning Java should be installed and configured correctly on the system. 
Java can be downloaded from the official website if necessary.

3. **Maven:** To manage dependencies and build projects effectively,
it will be necessary to have Maven installed on the system. 
Maven can be downloaded and installed from its official website or using package 
managers provided by the operating system.

**Note:**
While these are the minimum requirements for running Py_Lite_Compiler it's recommended to have
a more capable system for better performance, especially when dealing with larger codebases. 
Additionally, ensuring that the Java and Maven installations are up-to-date will help in avoiding
compatibility issues and leveraging the latest features and bug fixes.

---

## Technologies Used

Py_lite_compiler has been developed using a combination of technologies to ease versatility. 
Below, it's provided an overview of the key technologies used during the development process:

### Programming Language

- **OpenJDK 20.0.2:** The core of the analyzer is built upon the OpenJDK 20.0.2 platform. 
This widely recognized and open-source implementation of the Java Platform, 
Standard Edition (Java SE), provides the foundation for the application's functionality. 
OpenJDK ensures platform independence, making the analyzer compatible with various operating systems.

### User Interface Framework

- **JavaFX:** The graphical user interface (GUI) of the application is developed using JavaFX.
JavaFX is a powerful and modern UI framework that allows for the creation of interactive 
and visually appealing user interfaces. With JavaFX, users can expect a seamless and 
intuitive experience while interacting with the application.

### Dependency and Package Manager

- **Maven:** Maven has been used as the dependency and package manager for this project. 
Maven simplifies the management of project dependencies and facilitates the building process. 
It ensures that all required libraries and dependencies are seamlessly integrated into the project,
making it easier to maintain and distribute.

### Integrated Development Environment (IDE)

- **IntelliJ IDEA Ultimate:** The development of Py_lite_compiler was carried out using 
IntelliJ IDEA Ultimate. This professional-grade integrated development environment offers 
a wide range of features and tools to streamline the development process, including code 
editing, debugging, version control integration, and project management.


These technologies have been chosen for their reliability, performance, and cross-platform 
compatibility, to enhance the development process, improve code quality, and ensure the overall 
accuracy of Py_lite_compiler. The use of OpenJDK ensures that the analyzer can run on a wide 
range of operating systems, while JavaFX provides a robust and user-friendly interface for 
both experienced developers and newcomers to language analysis. The combination of these 
technologies results in a user-friendly, accessible and scalable tool for code analysis.

### External Libraries

In addition to the core technologies mentioned earlier, Py_lite_compiler relies on several 
external libraries to enhance its functionality and provide specific features. 
Below, are listed and briefly described these libraries:

- **Kordamp Bootstrapfx:** This library provides adapted Bootstrap styles for JavaFX, 
allowing the development of visually appealing and responsive user interfaces. 
It enhances the overall look and feel of the analyzer's graphical interface, 
providing a modern and professional appearance.

- **Project Lombok:** Lombok is a powerful library that simplifies Java code by generating 
repetitive code constructs (e.g., getters, setters, constructors) during the compilation phase. 
This not only reduces boilerplate code but also enhances code readability and maintainability.

- **RichTextFX:** The FXMisc RichText library offers a range of tools for creating text editors 
in JavaFX applications. The CodeArea component from this library was used to implement the 
text editor feature in the analyzer, making it easier for users to edit and analyze code.

- **Graphviz-Java - guru.nidi:** This library was utilized to enable the generation of graphics 
representing certain tokens and their lexemes. It provides the capability to create visual
representations of code structures, aiding in the understanding and visualization of code analysis results.

These external libraries significantly contribute to the functionality and user experience of Py_lite_compiler. 
By leveraging these libraries, the analyzer is equipped with enhanced UI styles, code generation 
capabilities, advanced text editing features, and visual representation of code structures, 
making it a comprehensive tool for language analysis and development.

---

# Lexical Analysis

The Lexical Analysis component of  Py_lite_compiler is responsible for scanning the code 
written in the CodeArea. It converts the code into a character array and reads it position 
by position to build tokens. This phase is critical for recognizing the fundamental building 
blocks of the code. The lexical analyzer identifies the following types of tokens:

## Token Types

1. **Literal Values:**
   - String
   - Integer
   - Float
   - Float with Exponents
   - Imaginary

2. **Identifiers:** These tokens represent variable and function names used in the code.

3. **Indentation and Dedentation:** The lexical analyzer efficiently manages the scopes and 
blocks of code by generating INDENT and DEDENT tokens. The indentation levels are tracked using 
a stack. The process is as follows:
   - Before reading the first line of the file, a single zero is pushed onto the stack.
   - At the beginning of each line, the line's indentation level is compared to the top of the stack.
   - If it is equal, nothing happens.
   - If it is larger, it is pushed onto the stack, and one INDENT token is generated.
   - If it is smaller, it must match one of the numbers occurring on the stack.
   - All numbers on the stack that are larger are popped off, and for each number popped off, a DEDENT token is generated.
   - At the end of the file, a DEDENT token is generated for each number remaining on the stack that is larger than zero.

4. **Keywords:** The lexical analyzer generates a distinct token for each reserved word of the Python language.

5. **Operators:** These tokens encompass arithmetic operation symbols and comparison symbols.

6. **Delimiters:** Delimiter tokens include assignment symbols, grouping symbols, 
and other similar symbols used to structure the code.

## Lexical Analysis Errors

The Lexical Analysis phase can potentially encounter two types of errors:

1. **Unrecognized Symbol or String:** This error occurs when the lexical analyzer encounters a 
symbol or string that doesn't match any of the expected expressions or tokens defined for the 
language. It indicates an INVALID_CHARACTER error in the code.

2. **Inconsistent Dedent Error:** In the context of managing indentation levels, this error is 
generated when the indentation of a line is larger than the top of the stack but does not match 
any level in the stack. This inconsistency makes it impossible to determine how to generate 
DEDENT tokens correctly. The Inconsistent Dedent Error signals a problem with the code's 
indentation and block structure.

The Lexical Analysis is an important step in the code analysis process, as it breaks down the code
into its fundamental components, enabling further syntactic analysis and interpretation of the 
code's structure and logic.

---

# Syntax Analyzer

The Syntax Analyzer the next important component of the program, responsible for interpreting 
and validating the structure of the code. It recognizes a set of simple assignments, control 
structures, and expressions while enforcing the rules of the language's grammar. 
Here's an overview of the supported syntax:

## Supported Syntax Elements

1. **Simple Assignments:** The syntax analyzer recognizes simple assignments, which include 
expressions, `pass`, `break`, `continue`, and `return` statements.

2. **Control Structures:** Supported control structures include:
   - `if`, `if-else`, `if-elif-else`
   - `for`, `for-else`
   - `while`, `while-else`

3. **Function Definitions:** The syntax analyzer can parse function definitions within the code.

4. **Expressions:** Expressions encompass all arithmetic operations and comparisons supported 
by Python, including objects like tuples, groups, lists, dictionaries, and sets.

## Syntax Analysis Process

The syntax analysis process begins by receiving the list of tokens generated by the lexical 
analyzer. The parser relies on a set of production enums provided by ProdFCTY, simulating the 
structure of each production in the grammar designed for the analyzer.

The analysis follows these steps:

1. **Generating 'Statements':** The initial goal of the syntax analysis is to generate the 'statements' 
production, from which all other productions are derived. It does this by employing a recursive method.

2. **Recursive Parsing:** The recursive method aims to match various options within the production, 
managing them on a stack. It attempts to generate each of the required productions and removes 
those that match from the stack, and continues doing it until the stack is empty or an element doesn't
match, jumping to the next option if there is or throwing an error if there are no more options to try to match.

3. **Consuming Tokens:** The analysis iterates through the tokens, following this cycle until a 
production matches, or an error is encountered.

4. **Storing Data:** This is specifically for report management. For now, the reports generated 
by the parser are divided into two main classes: on blocks and on functions. So for this, the parser, 
at the same time that it runs the analysis, it also collects the information that will be necessary
to show these reports later.

## Syntax errors

The Syntax Analyzer is designed to provide feedback when it encounters errors. 
Although the current feedback may not be extremely detailed, it offers enough information to help 
locate issues within the code. Common errors may include mismatched control structures, incorrect 
use of expressions, or improperly defined function definitions.

While the application currently focuses on a subset of Python-inspired language features, 
it serves as a foundational step towards achieving a comprehensive code analysis tool, with 
the possibility to support a broader range of language constructs in the future.

---

## User Interface Overview

Here's a breakdown of the features and functionalities on the user-interface:

### Code Editor

- **Location:** The code editor occupies the left half of the application window, featuring a 
CodeArea that allows to write and modify the code.

- **Default Color:** The code text color is black.

- **Line Enumeration:** Line numbers are enumerated, making it easy to navigate and reference specific lines of code.

#### Top Toolbar

Located just above the CodeArea, the top toolbar offers the functions:

- **File Button (Yellow):** This button allows to load code from a file with the extensions 
`.txt` or `.py`. The content of the file will be automatically added to the CodeArea.

- **Run Button (Green):** Clicking this button will initiate code analysis, which includes both 
lexical and syntactic analysis. The results of the lexical analysis will be also displayed in the 
CodeArea with the following code highlighting:
   - Identifiers: White
   - Keywords: Purple
   - Arithmetic, Logical, and Assignment Operators: Cyan
   - Literals: Orange
   - Other Symbols: Green
   - Comments: Black

### Reports

The right half of the window is used to display and access to al the generated reports, this section is 
divided into three rows, each serving a distinct purpose:

#### Token Table (row 1)

- **Content:** The token table displays information about each generated token, including the 
token type, pattern, lexeme, and its location on the code (line and column).

- **Graph Button (Blue):** By selecting a token in the table and clicking this button, a small 
pop-up window will display a graph generated using Graphviz for the selected token's lexeme.

#### Output Space (row 2)

- **Content:** This area provides feedback and messages related to the analysis. 
It informs you if the code analysis completed successfully or if errors were encountered. 
If errors exist, they will be listed and described here.

#### Functions and Blocks Tables (row 3)

- **Functions Table (Right):** Lists all functions found in the code, the beginning line, end line and it's identifier.

- **Blocks Table (Left):** Lists all code blocks detected in the code and the lines where the bloc starts and ends.

- **Symbol Table Button (Blue):** Above the Blocks Table, the "Symbol Table" button displays 
ins a pop-up window the symbol table or a list of statements corresponding to a selected block.

- **Statements Button (Blue):** Similar to the Symbol Table button, it displays the 
list of statements associated with a selected block.

- **Show Params Button (Blue):** Located above the Functions Table, this button opens a 
pop-up window displaying a list of parameters for the selected function.

The user interface streamlines the code analysis process, providing code highlighting, 
error feedback, and easy access to essential information for effective analysis and debugging.


